---
title: 'Estimadores y su distribución. Inferencia'
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    number_sections: yes
    toc: yes
---

# Inferencia de parámetros individuales

## Distribución asintótica de los estimadores

Para muestras grandes, los estimadores de máxima verosimilitud tienen distribución asintótica normal. En concreto, se tiene que

$$
\hat \beta \sim N(\beta, I(\hat \beta))
$$

donde $I(\beta)$ se denomina Matriz de Información de Fisher observada:

$$
I(\beta) = -H_{logL}^{-1}(\beta)
$$
es decir, la inversa del hessiano de la función de verosimilitud (con signo negativo):

$$
H_{logL}(\beta) = - X^T W X
$$

En el caso de la propiedad anterior, la matriz $I(\beta)$ está evaluada en el valor que maximiza la verosimilitud, $I(\hat \beta)$.

Por tanto, cada estimador de manera individual se distribuye como:

$$
\hat \beta_j \sim N(\beta_j, Var(\hat \beta_j))
$$

donde 

$$
Var(\hat \beta_j) = I_{(j+1,j+1)}(\hat \beta), \quad j = 0,1,\ldots,k
$$

es decir, los elementos de la diagonal de la matriz $I(\hat \beta)$. Al igual que en regresión lineal, el *standard error* de los estimadores es:

$$
se(\hat \beta_j) = \sqrt{Var(\hat \beta_j)}
$$

## Contrastes de hipótesis individuales

Para resolver contrastes del tipo:

$$
H_0 : \beta_j = 0 \\
H_1 : \beta_j \neq 0
$$

se utiliza la distribución asintóntica mostrada anteriormente:

$$
\frac{\hat \beta_j - \beta_j}{se(\hat \beta_j)} \sim N(0,1)
$$

Por tanto, si la hipótesis nula es cierta se tiene:

$$
\frac{\hat \beta_j}{se(\hat \beta_j)} \sim N(0,1)
$$

A este método (contraste de hipótesis utilizando la distribución asintótica de los estimadores) se lo conoce como método de Wald, o estadístico de [Wald](https://en.wikipedia.org/wiki/Abraham_Wald).

Con R:

```{r}
d = read.csv("datos/MichelinNY.csv")
str(d)
#d$InMichelin = factor(d$InMichelin)
```

Estimamos los parámetros del modelo (se van a utilizar las funciones de R del archivo [logit_funciones.R](logit_funciones.R)):

```{r}
# cargamos las funciones que vamos a utilizar
source("funciones/logit_funciones.R")

# punto de partida
m = lm(InMichelin ~ Food + Decor + Service + Price, data = d)
beta_i = coef(m)

# matriz X
X = cbind(rep(1,nrow(d)), d[,3:6])

# estimacion con el algoritmo optim
mle = optim(par = beta_i, fn = logit_logL_optim, gr = NULL,
  y = d$InMichelin, X = X, 
  method = "BFGS", hessian = F, 
  control = list(trace = 1, REPORT = 1, maxit = 200))
(beta_e = mle$par)
```

```{r}
# matriz de información de Fisher
I = -solve(logit_hess(beta_e,X))
# standard error de los parámetros estimados
(beta_se = sqrt(diag(I)))
```

```{r}
# valor del estadístico del contraste
(z = beta_e/beta_se)
```

```{r}
# pvalores
2*(1 - pnorm(abs(z)))
```

Esta información se obtiene cuando utilizamos la funcion glm():

```{r}
m1 = glm(InMichelin ~ Food + Decor + Service + Price, data = d, family = binomial)
summary(m1)
```

## Intervalos de confianza

Partimos de nuevo del estadístico de Wald:

$$
\frac{\hat \beta_j - \beta_j}{se(\hat \beta_j)} \sim N(0,1)
$$

Por tanto, el intervalo será:

$$
\hat \beta_j - z_{\alpha/2}se(\hat \beta_j) \leq \beta_j \leq \hat \beta_j + z_{\alpha/2}se(\hat \beta_j)
$$

```{r}
alfa = 0.05
data.frame(LI = beta_e - qnorm(1-alfa/2)*beta_se,
LS = beta_e + qnorm(1-alfa/2)*beta_se)
```

```{r}
confint(m1, level = 1-alfa)
```

