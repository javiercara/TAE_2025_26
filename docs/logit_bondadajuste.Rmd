---
title: "Bondad de ajuste"
output: 
  html_document:
    number_sections: true
    toc: true
    toc_float: true
  pdf_document:
    number_sections: true
    toc: true
---

# Introduccion

Se estima el siguiente modelo de regresión logística:

```{r}
d = read.csv("datos/MichelinNY.csv")
m1 = glm(InMichelin ~ Food + Decor + Service + Price, data = d, family = binomial)
summary(m1)
```
El objetivo es analizar como de bueno es el modelo de regresión logística que se ha estimado.

# Criterio de la matriz de confusión

El método más sencillo es calcular el error de predicción del modelo en la base de datos. Esto se hace con la matriz de confusión.

```{r}
pred_prob = predict(m1, newdata = d, type = "response")
n = nrow(d)
pred_y = rep(0, n)
pred_y[pred_prob > 0.5] = 1
# matriz de confusion
(t = table(d$InMichelin, pred_y))
```

Por tanto, se han predicho bien `r t[1,1]` + `r t[2,2]` = `r t[1,1] + t[2,2]` datos de un todal de `r n`. Se han predicho mal `r t[1,2]` + `r t[2,1]` = `r t[1,2] + t[2,1]` datos de un todal de `r n`. El error del modelo es `r t[1,2] + t[2,1]` / `r n` = `r round((t[1,2] + t[2,1])/n*100,2)`%.

Cuando el objetivo de principal de la regresión logística sea la predicción, la bondad del modelo se puede calcular construyendo la matriz de confusión en un test set:

```{r}
set.seed(123)
pos_train = sample(1:n, round(0.8*n), replace = F)
train = d[pos_train,]
test = d[-pos_train,]
```

```{r}
m2 = glm(InMichelin ~ Food + Decor + Service + Price, data = train, family = binomial)
test_prob = predict(m2, newdata = test, type = "response")
n_test = nrow(test)
pred_y = rep(0, n_test)
pred_y[test_prob > 0.5] = 1
# matriz de confusion
(t = table(test$InMichelin, pred_y))
```

# R-cuadrado en regresión logística

Otra manera de calcular la bondad del modelo es definir un $R^2$ de manera similar a como se hizo en regresión lineal. Se han propuesto muchas formas de definir este $R^2$, pero quizá la más usada es:

$$
R^2 = 1 - \frac{D_1}{D_0}
$$
donde D es la desviación del modelo (deviance en inglés). Se define como el doble de la verosimilitud del modelo calculada en los parámetros estimados (en valor absoluto):

$$
D = |2logL(\hat \beta)|
$$

$$
log L(\hat \beta) = \sum_{i=1}^{n}(y_i log \hat \pi_i +  (1-y_i) log(1 - \hat \pi_i))
$$

$$
\hat \pi_i = \frac{exp(x_i^T \hat \beta)}{1 + exp(x_i^T \hat \beta)}
$$

Se definen dos desviaciones:

- D1: la desviación del modelo analizado.
- D0: la desviación del modelo en el que solo se estima $\beta_0$.

```{r}
source("funciones/logit_funciones.R")
(D1 = abs(2*logit_logL(coef(m1),d$InMichelin,model.matrix(m1))) )
```

```{r}
m0 = glm(InMichelin ~ 1, data = d, family = binomial)
summary(m0)
```

```{r}
(D0 = abs(2*logit_logL(coef(m0),d$InMichelin,model.matrix(m0))) )
```

```{r}
(R2 = 1 - D1/D0)
```

Si $R^2 \approx 1$ el modelo se ajusta muy bien a los datos, y $R^2 \approx 0$ implica un mal ajuste. Es decir, $R^2 \approx 0$ significa que la verosimilitud de ambos modelos es muy parecida, luego $\beta_1 \approx \beta_2 \approx \beta_k \approx 0$.

# Contraste para un grupo de coeficientes

Supongamos que tenemos dos modelos:

$$
\pi_i = \frac{exp(x_i^T \beta)}{1 + exp(x_i^T \beta)}
$$

$$
\pi_{Ai} = \frac{exp(x_{Ai}^T \beta_A)}{1 + exp(x_{Ai}^T \beta_A)}
$$

donde $\beta_A$ es un subconjunto de $\beta$. Supongamos que $dim(\beta_A) = m$ y $dim(\beta) = k$, con $m<k$. Si $\beta_B$ representa los parámetros que están en $\beta$ pero no están en $\beta_A$, se puede resolver el siguiente contraste:

$$ H_0 : \beta_B = 0, \quad H_1: \beta_B \neq 0 $$

En el caso de que la hipótesis nula sea cierta, se tiene que:

$$
G = D_A - D_1 \sim \chi^2_{k-m}
$$

donde $D_1$ es la desviación del modelo con parámetros $\beta$ y $D_A$ es la desviación del modelo con parámetros $\beta_A$.

- Si $G \geq \chi^2_{\alpha}$ se rechaza la hipótesis nula.
- Si $G < \chi^2_{\alpha}$ no se rechaza la hipótesis nula.

Es decir, valores grandes del estadístico significa que la verosimilitud de ambos modelos es muy diferente, luego $\beta_B \neq 0$ por lo que se tiene que rechazar $H_0$. Para valores pequeños de G, ambos modelos son muy parecidos, luego los regresores $\beta_B$ no aportan nada al modelo, es decir, $\beta_B = 0$, no se rechaza $H_0$.

Por ejemplo, queremos resolver el contraste con hipótesis nula $\beta_1 = \beta_2 = 0$:

```{r}
mA = glm(InMichelin ~ Service + Price, data = d, family = binomial)
(DA = abs(2*logit_logL(coef(mA),d$InMichelin,model.matrix(mA))) )
(G = DA - D1)
# valor crítico del contraste
k = length(coef(m1))
m = length(coef(mA))
qchisq(0.95, df = k-m)
```

Luego se rechaza la hipótesis nula.

# Contraste de bondad de ajuste

Utilizando el contraste anterior entre el modelo con todos los regresores y el modelo con solo $\beta_0$ se puede analizar la bondad del modelo. Es decir, se puede contrastar:

- H0: el modelo estimado NO es adecuado ($\beta_1 = \beta_2 = \cdots = \beta_k = 0$)
- H1: el modelo estimado es adecuado.

El estadístico del contraste es

$$
G = D_0 - D_1 \sim \chi^2_{k-1}
$$
En este caso:

```{r}
(G = D0 - D1)
k = length(coef(m1))
(pvalor = 1-pchisq(G, k-1))
```

Luego el modelo es muy adecuado.